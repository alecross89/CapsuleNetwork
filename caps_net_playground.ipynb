{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some interesting questions:**\n",
    "\n",
    " - More routing iterations: Does more routing help classification. (This routing operation is sweet, can we benefit from it more)\n",
    " \n",
    " - Varying the number of capsules: Does the number(32)/size(8) of the PrimaryCaps layer help classification? (This network is heavy AF)\n",
    " \n",
    " - More initial convolutional layers: Do smaller filters (3x3, say) with more standard conv layers help classification? (This network is heavy AF)\n",
    " \n",
    " - No primary capsules: Can we get away with just convolutional layers, routing, DigiCaps and reconstruction?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Matrix Multiplications Before Routing\n",
    "Here we're working out the logic for the large number of matrix multiplications that need to happen before the routing procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- _->1 ----------------\n",
      "\n",
      "shape of U_: (2, 2, 1, 3)\n",
      "shape of W_jSingle: (2, 3, 2)\n",
      "shape of W_j: (2, 2, 3, 2)\n",
      "shape of UHatj_: (2, 2, 2) \n",
      "\n",
      "\n",
      "matmul output: \n",
      "[[[  3.   3.]\n",
      "  [ 12.  12.]]\n",
      "\n",
      " [[ 21.  21.]\n",
      "  [ 66.  66.]]]\n",
      "\n",
      "---------------- _->2 ----------------\n",
      "\n",
      "shape of U_: (2, 2, 1, 3)\n",
      "shape of W_jSingle: (2, 3, 2)\n",
      "shape of W_j: (2, 2, 3, 2)\n",
      "shape of UHatj_: (2, 2, 2) \n",
      "\n",
      "\n",
      "matmul output: \n",
      "[[[   9.    9.]\n",
      "  [  30.   30.]]\n",
      "\n",
      " [[  63.   63.]\n",
      "  [ 165.  165.]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "batch_size = 2        # typically: 64\n",
    "num_of_caps_ops = 2   # typically: 32*6*6\n",
    "caps_op_size = 3      # typically: 8\n",
    "num_of_digi_caps = 2  # typically: 10 (b/c there are 10 digits/classes)\n",
    "digi_caps_size = 2    # typically: 16\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "U = np.array([     # Notice U ~ (2, 2, 1, 3) ~ (batch_size, num_of_caps_ops, 1, caps_op_size)\n",
    "              [\n",
    "               [[1, 1, 1]], \n",
    "               [[2, 2, 2]]\n",
    "              ],\n",
    "              [\n",
    "               [[7, 7, 7]], \n",
    "               [[11, 11, 11]]\n",
    "              ]\n",
    "             ])\n",
    "W_single = [ np.array([   # Notice W_j ~ (2, 3, 2) ~ (num_of_caps_ops, caps_op_size, digi_caps_size)\n",
    "                       [[1, 1],\n",
    "                        [1, 1],\n",
    "                        [1, 1]],\n",
    "                       [[2, 2],\n",
    "                        [2, 2],\n",
    "                        [2, 2]]\n",
    "                      ]),\n",
    "              np.array([   # Notice W_j ~ (2, 3, 2) ~ (num_of_caps_ops, caps_op_size, digi_caps_size)\n",
    "                       [[3, 3],\n",
    "                        [3, 3],\n",
    "                        [3, 3]],\n",
    "                       [[5, 5],\n",
    "                        [5, 5],\n",
    "                        [5, 5]]\n",
    "                      ])\n",
    "           ]\n",
    "\n",
    "# U = np.random.normal(size=(batch_size, num_of_caps_ops, 1, caps_op_size))\n",
    "# W_j_single = np.random.normal(size=(num_of_caps_ops, caps_op_size, digi_caps_size))\n",
    "\n",
    "U_ = tf.get_variable('U_', initializer=tf.constant(U))\n",
    "U_ = tf.cast(U, tf.float32)\n",
    "UHat = []\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "# basically U_ will be given to us as the output of the primary caps layer,\n",
    "# from there we repeat the proceedure below 10 times, for \n",
    "# j in {1, 2, ..., 10}, namely we should have (distinct) tensors:\n",
    "# W_1Single, W_2Single, ..., W_10Single.\n",
    "for i in range(1, num_of_digi_caps + 1):\n",
    "#     tf.reset_default_graph()\n",
    "#     with tf.variable_scope('primary_caps_TO_{}'.format(i)):\n",
    "    W_jSingle = tf.get_variable('W{}jSingle'.format(i), initializer=tf.constant(W_single[i - 1]))\n",
    "    W_jSingle = tf.cast(W_jSingle, tf.float32)\n",
    "\n",
    "\n",
    "    # here we want to reuse W_jSingle to carry out the matrix multiplication\n",
    "    # for each item (primary caps output) in the batch\n",
    "    temp = []\n",
    "    for _ in range(1, batch_size + 1):\n",
    "        temp.append(W_jSingle)\n",
    "    W_j = tf.stack(temp)\n",
    "\n",
    "    UHatj_ = tf.reshape(tf.matmul(U_, W_j), (batch_size, num_of_caps_ops, digi_caps_size))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        print('---------------- _->{} ----------------\\n'.format(i))\n",
    "        print('shape of U_: {}'.format(sess.run(U_).shape))\n",
    "        print('shape of W_jSingle: {}'.format(sess.run(W_jSingle).shape))\n",
    "        print('shape of W_j: {}'.format(sess.run(W_j).shape))\n",
    "        print('shape of UHatj_: {}'.format(sess.run(UHatj_).shape), '\\n')\n",
    "        print('\\nmatmul output: \\n{}\\n'.format(sess.run(UHatj_)))\n",
    "        UHat.append(sess.run(UHatj_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[   3.    3.]\n",
      "   [  12.   12.]]\n",
      "\n",
      "  [[   9.    9.]\n",
      "   [  30.   30.]]]\n",
      "\n",
      "\n",
      " [[[  21.   21.]\n",
      "   [  66.   66.]]\n",
      "\n",
      "  [[  63.   63.]\n",
      "   [ 165.  165.]]]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.stack(UHat, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so this way of doing things lookes like it's exhibiting the desired behaviour. In particular this should be able to handle batches of arbitrary size.\n",
    "\n",
    "Next up: routing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing With Batches\n",
    "Here we work out the routing procedure in the presence of batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect all of the $\\widehat{u}_{j \\vert *}$ into one tensor, stacking along `axis = 1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UHat = tf.stack(UHat, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the routing we'll need a modified softmax function which can take the softmax along a specified axis. Note, `tf.nn.softmax` does not give the desired behavior in this specific instance, since `tf.nn.softmax` actually performs the softmax over the right-most (last?) axis.\n",
    "\n",
    "Also, notice that \n",
    "\n",
    "$$softmax(x) = softmax(x - c), \\quad c \\in \\mathbf{R}$$\n",
    "\n",
    "Thus taking $c$ to be the max along the specified axis means that the largest term we ever exponentiate by is 0 (i.e., most terms appearing in the exponent will be negative), this is done with numerical stability in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(inbound, axis=0, name=None):\n",
    "    with tf.name_scope(name, 'softmax', [inbound]) as scope:\n",
    "        # first we find the max entry along the specified axis\n",
    "        max_along_axis= tf.reduce_max(inbound, axis, keep_dims=True)\n",
    "        \n",
    "        # then subtract the max from all entries to \n",
    "        # help with numerical stability\n",
    "        exp = tf.exp(inbound - max_along_axis)                       \n",
    "        \n",
    "        # next we compute the term used for normalization\n",
    "        normalizing_term = tf.reduce_sum(exp, axis, keep_dims=True)  \n",
    "        \n",
    "        # lastly, compute the softmax (along the specified axis)\n",
    "        softmax = exp / normalizing_term               \n",
    "        \n",
    "        return tf.identity(softmax, name=scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we would like to take softmaxes over the \"columns\" `[1, 1]`, `[3, 5]`, `[7, 13]`, and `[2, 11]`, instead of over the \"rows\" `[1, 3]`, `[1, 5]`, `[7, 2]`, `[13, 11]` as would be done if we used `tf.nn.softmax`. \n",
    "\n",
    "Of course we could do this with `tf.transpose` and subsequently use the built-in `tf.nn.softmax`, but this would involve transposing, softmax-ing, and then transposing back (given the way we've set things up so far), so a small extra function for customized softmaxes seems reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test tensor: \n",
      "\n",
      "[[[[  1.   3.]]\n",
      "\n",
      "  [[  1.   5.]]]\n",
      "\n",
      "\n",
      " [[[  7.   2.]]\n",
      "\n",
      "  [[ 13.  11.]]]]\n",
      "\n",
      "custom softmax: (the desired behaviour) \n",
      "\n",
      "[[[[  5.00000000e-01   1.19202919e-01]]\n",
      "\n",
      "  [[  5.00000000e-01   8.80797029e-01]]]\n",
      "\n",
      "\n",
      " [[[  2.47262325e-03   1.23394580e-04]]\n",
      "\n",
      "  [[  9.97527421e-01   9.99876618e-01]]]]\n",
      "\n",
      "built-in softmax: \n",
      "\n",
      "[[[[ 0.11920291  0.88079703]]\n",
      "\n",
      "  [[ 0.01798621  0.98201376]]]\n",
      "\n",
      "\n",
      " [[[ 0.99330717  0.00669285]]\n",
      "\n",
      "  [[ 0.88079703  0.11920291]]]]\n"
     ]
    }
   ],
   "source": [
    "xx = tf.reshape(tf.constant([[[[1, 3]], \n",
    "                              [[1, 5]]], \n",
    "                             [[[7, 2]], \n",
    "                              [[13, 11]]]]), shape=(2, 2, 1, 2))\n",
    "xx = tf.cast(xx, tf.float32)\n",
    "with tf.Session() as sess:\n",
    "    print('test tensor: \\n')\n",
    "    print(sess.run(xx))\n",
    "    print('\\ncustom softmax: (the desired behaviour) \\n')\n",
    "    print(sess.run(softmax(xx, axis=1)))\n",
    "    print('\\nbuilt-in softmax: \\n')\n",
    "    print(sess.run(tf.nn.softmax(xx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the softmax out of the way we get to the actual routing algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building the Capsule Network\n",
    "Now We start building the Capsule Network. We begin with a function to shorten up the call to `tf.layers.conv2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(inputs, filters, kernel_size=9, strides=1, padding='valid', activation=None):\n",
    "    return tf.layers.conv2d(inputs=inputs, \n",
    "                            filters=filters, \n",
    "                            kernel_size=kernel_size, \n",
    "                            strides=strides, \n",
    "                            padding=padding, \n",
    "                            activation=activation, \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "ROUTING_ITERS = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones = tf.reshape(tf.ones([784]), [-1, 28, 28, 1])\n",
    "out = conv_layer(ones, 256, activation=tf.nn.relu)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output = sess.run(out)\n",
    "    data_shape = output.shape\n",
    "    assert output.shape == (1, 20, 20, 256), 'Incorrect shape, {}, after regular conv layer. Should be {}'.format(data_shape, [1, 20, 20, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caps_net_model_fn(features, labels, mode):\n",
    "    \n",
    "    # image layer\n",
    "    assert features['x'].get_shape()[1:] == [784] \n",
    "    input_layer = tf.reshape(features['x'], shape=(-1, 28, 28, 1))\n",
    "    \n",
    "    # save the batch size for later (<= 64)\n",
    "    batch_size = input_layer.get_shape()[0]\n",
    "    n_primary_caps = 32\n",
    "    primary_caps_size = 8\n",
    "    digi_caps_size = 16\n",
    "    \n",
    "    # conv layer (regular convolutional layer)\n",
    "    with tf.variable_scope('regular_conv_layer'):\n",
    "        conv1 = conv_layer(input_layer, 256, activation=tf.nn.relu)\n",
    "        data_shape = conv1.get_shape()\n",
    "        e1 = 'Incorrect shape, {}, after regular conv layer. Should be {}'.format(data_shape, [batch_size, 20, 20, 256])\n",
    "        assert data_shape == [batch_size, 20, 20, 256], e1\n",
    "    \n",
    "    # first capsule layer (PrimaryCaps)\n",
    "    capsules = []\n",
    "    for i in range(32):\n",
    "        # naming convention: capsule_[capsule layer]_[capsule index]\n",
    "        with tf.variable_scope('capsule_1_' + str(i + 1)):\n",
    "            caps_i = conv_layer(conv1, primary_caps_size, strides=2)\n",
    "            reshape = tf.reshape(caps_i, shape=(-1, 6*6, primary_caps_size))\n",
    "            capsules.append(reshape)\n",
    "    assert capsules[0].get_shape() == [batch_size, 6*6, primary_caps_size]\n",
    "    \n",
    "    # stack and reshape\n",
    "    #\n",
    "    # here we reshape the capsule outputs (i.e. the 8D vectors)\n",
    "    # to be 1x8 matrices, this will enable us to use tf.matmul\n",
    "    # to calculate the inputs (u_hat_ij's) to each DigiCaps\n",
    "    # capsule in one shot\n",
    "    capsules = tf.stack(capsules, axis=1)\n",
    "    capsules = tf.reshape(capsules, shape=(-1, 6*6*n_primary_caps, 1, primary_caps_size))\n",
    "    assert capsules.get_shape() == [batch_size, 6*6*n_primary_caps, 1, primary_caps_size]\n",
    "    \n",
    "    # second capsule layer (DigiCaps)\n",
    "    u_hat = []\n",
    "    for j in range(10):\n",
    "        with tf.variable_scope('->capsule_{}'.format(j)):\n",
    "            name = 'W_i{}'.format(j)\n",
    "            weights_to_j = tf.get_variable(name=name, shape=(1, 6*6*n_primary_caps, primary_caps_size, digi_caps_size))\n",
    "            weights_to_j = tf.tile(weights_to_j, (batch_size, 1, 1, 1))\n",
    "            u_hat_ji = tf.matmul(capsules, weights_to_j, )\n",
    "            \n",
    "    \n",
    "    \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
