{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - More routing iterations: Does more routing help classification. (This routing operation is sweet, can we benefit from it more)\n",
    " \n",
    " - Varying the number of capsules: Does the number(32)/size(8) of the PrimaryCaps layer help classification? (This network is heavy as f!@#)\n",
    " \n",
    " - More initial convolutional layers: Do smaller filters (3x3, say) with more standard conv layers help classification? (This network is heavy as f!@#)\n",
    " \n",
    " - No primary capsules: Can we get away with just convolutional layers, routing, DigiCaps and reconstruction?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Matrix Multiplications Before Routing\n",
    "Here we're working out the logic for the large number of matrix multiplications that need to happen before the routing procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of U_: (2, 2, 1, 3)\n",
      "shape of W_jSingle: (2, 3, 2)\n",
      "shape of W_j: (2, 2, 3, 2)\n",
      "shape of UHatj_: (2, 2, 2) \n",
      "\n",
      "desired output: \n",
      "[[[  3.   3.]\n",
      "  [ 12.  12.]]\n",
      "\n",
      " [[  9.   9.]\n",
      "  [ 24.  24.]]]\n",
      "\n",
      "matmul output: \n",
      "[[[  3.   3.]\n",
      "  [ 12.  12.]]\n",
      "\n",
      " [[  9.   9.]\n",
      "  [ 24.  24.]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "batch_size = 2      # typically: 64\n",
    "num_of_caps_ops = 2 # typically: 32*6*6\n",
    "caps_op_size = 3    # typically: 8\n",
    "digi_caps_size = 2  # typically: 16\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "U = np.array([     # Notice U ~ (2, 2, 1, 3) ~ (batch_size, num_of_caps_ops, 1, caps_op_size)\n",
    "              [\n",
    "               [[1, 1, 1]], \n",
    "               [[2, 2, 2]]\n",
    "              ],\n",
    "              [\n",
    "               [[3, 3, 3]], \n",
    "               [[4, 4, 4]]\n",
    "              ]\n",
    "             ])\n",
    "W_j_single = np.array([   # Notice W_j ~ (2, 3, 2) ~ (num_of_caps_ops, caps_op_size, digi_caps_size)\n",
    "                       [[1, 1],\n",
    "                        [1, 1],\n",
    "                        [1, 1]],\n",
    "                       [[2, 2],\n",
    "                        [2, 2],\n",
    "                        [2, 2]]\n",
    "                      ])\n",
    "\n",
    "# U = np.random.normal(size=(batch_size, num_of_caps_ops, 1, caps_op_size))\n",
    "# W_j_single = np.random.normal(size=(num_of_caps_ops, caps_op_size, digi_caps_size))\n",
    "\n",
    "U_ = tf.get_variable('U_', initializer=tf.constant(U))\n",
    "U_ = tf.cast(U, tf.float32)\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "# basically U_ will be given to us as the output of the primary caps layer,\n",
    "# from there we repeat the proceedure below 10 times, for \n",
    "# j in {1, 2, ..., 10}, namely we should have (distinct) tensors:\n",
    "# W_1Single, W_2Single, ..., W_10Single.\n",
    "W_jSingle = tf.get_variable('W_jSingle', initializer=tf.constant(W_j_single))\n",
    "W_jSingle = tf.cast(W_jSingle, tf.float32)\n",
    "\n",
    "# here we want to reuse W_jSingle to carry out the matrix multiplication\n",
    "# for each item (primary caps output) in the batch\n",
    "temp = []\n",
    "for i in range(1, batch_size + 1):\n",
    "    temp.append(W_jSingle)\n",
    "W_j = tf.stack(temp)\n",
    "    \n",
    "UHatj_ = tf.reshape(tf.matmul(U_, W_j), (batch_size, num_of_caps_ops, digi_caps_size))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print('shape of U_: {}'.format(sess.run(U_).shape))\n",
    "    print('shape of W_jSingle: {}'.format(sess.run(W_jSingle).shape))\n",
    "    print('shape of W_j: {}'.format(sess.run(W_j).shape))\n",
    "    print('shape of UHatj_: {}'.format(sess.run(UHatj_).shape), '\\n')\n",
    "    print('desired output: \\n{0}'.format(np.array([[[3., 3.],[12., 12.]],[[9., 9.],[24., 24.]]])))\n",
    "    print('\\nmatmul output: \\n{}'.format(sess.run(UHatj_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so this way of doing things lookes like it's exhibiting the desired behaviour. In particular this should be able to handle batches of arbitrary size.\n",
    "\n",
    "Next up: routing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing With Batches\n",
    "Here we work out the routing procedure in the presence of batches.\n",
    "\n",
    "Notice that `tf.slice` behaves as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5 6]\n",
      "  [7 8]]]\n",
      "[[[ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "M = tf.constant([[[1, 2], [3, 4]], \n",
    "                 [[5, 6], [7, 8]],\n",
    "                 [[9, 10], [11, 12]]])\n",
    "s1 = tf.slice(M, begin=[1, 0, 0], size=[1, 2, 2])\n",
    "s2 = tf.slice(M, begin=[1, 0, 0], size=[-1, 2, 2])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(s1))\n",
    "    print(sess.run(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Building the Capsule Network\n",
    "Now We start building the Capsule Network. We begin with a function to shorten up the call to `tf.layers.conv2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(inputs, filters, kernel_size=9, strides=1, padding='valid', activation=None):\n",
    "    return tf.layers.conv2d(inputs=inputs, \n",
    "                            filters=filters, \n",
    "                            kernel_size=kernel_size, \n",
    "                            strides=strides, \n",
    "                            padding=padding, \n",
    "                            activation=activation, \n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "ROUTING_ITERS = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = tf.reshape(tf.ones([784]), [-1, 28, 28, 1])\n",
    "out = conv_layer(ones, 256, activation=tf.nn.relu)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output = sess.run(out)\n",
    "    data_shape = output.shape\n",
    "    assert output.shape == (1, 20, 20, 256), 'Incorrect shape, {}, after regular conv layer. Should be {}'.format(data_shape, [1, 20, 20, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caps_net_model_fn(features, labels, mode):\n",
    "    \n",
    "    # image layer\n",
    "    assert features['x'].get_shape()[1:] == [784] \n",
    "    input_layer = tf.reshape(features['x'], shape=(-1, 28, 28, 1))\n",
    "    \n",
    "    # save the batch size for later (<= 64)\n",
    "    batch_size = input_layer.get_shape()[0]\n",
    "    n_primary_caps = 32\n",
    "    primary_caps_size = 8\n",
    "    digi_caps_size = 16\n",
    "    \n",
    "    # conv layer (regular convolutional layer)\n",
    "    with tf.variable_scope('regular_conv_layer'):\n",
    "        conv1 = conv_layer(input_layer, 256, activation=tf.nn.relu)\n",
    "        data_shape = conv1.get_shape()\n",
    "        e1 = 'Incorrect shape, {}, after regular conv layer. Should be {}'.format(data_shape, [batch_size, 20, 20, 256])\n",
    "        assert data_shape == [batch_size, 20, 20, 256], e1\n",
    "    \n",
    "    # first capsule layer (PrimaryCaps)\n",
    "    capsules = []\n",
    "    for i in range(32):\n",
    "        # naming convention: capsule_[capsule layer]_[capsule index]\n",
    "        with tf.variable_scope('capsule_1_' + str(i + 1)):\n",
    "            caps_i = conv_layer(conv1, primary_caps_size, strides=2)\n",
    "            reshape = tf.reshape(caps_i, shape=(-1, 6*6, primary_caps_size))\n",
    "            capsules.append(reshape)\n",
    "    assert capsules[0].get_shape() == [batch_size, 6*6, primary_caps_size]\n",
    "    \n",
    "    # stack and reshape\n",
    "    #\n",
    "    # here we reshape the capsule outputs (i.e. the 8D vectors)\n",
    "    # to be 1x8 matrices, this will enable us to use tf.matmul\n",
    "    # to calculate the inputs (u_hat_ij's) to each DigiCaps\n",
    "    # capsule in one shot\n",
    "    capsules = tf.stack(capsules, axis=1)\n",
    "    capsules = tf.reshape(capsules, shape=(-1, 6*6*n_primary_caps, 1, primary_caps_size))\n",
    "    assert capsules.get_shape() == [batch_size, 6*6*n_primary_caps, 1, primary_caps_size]\n",
    "    \n",
    "    # second capsule layer (DigiCaps)\n",
    "    u_hat = []\n",
    "    for j in range(10):\n",
    "        with tf.variable_scope('->capsule_{}'.format(j)):\n",
    "            name = 'W_i{}'.format(j)\n",
    "            weights_to_j = tf.get_variable(name=name, shape=(1, 6*6*n_primary_caps, primary_caps_size, digi_caps_size))\n",
    "            weights_to_j = tf.tile(weights_to_j, (batch_size, 1, 1, 1))\n",
    "            u_hat_ji = tf.matmul(capsules, weights_to_j, )\n",
    "            \n",
    "    \n",
    "    \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
